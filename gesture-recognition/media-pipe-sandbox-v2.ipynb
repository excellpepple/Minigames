{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "* this is a second version of the sandbox where i use media pipes tasks api calls instead of the older solutions version.\n",
    "* It shows how to use the tasks api to read hand landmarks\n",
    "*\n",
    "\"\"\""
   ],
   "id": "3e9970e692fffafc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import tensorflow as tf\n",
    "from tensorflow import timestamp\n",
    "from tensorflow.keras.models import load_model"
   ],
   "id": "7db6741d98ab18f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T21:28:26.588617Z",
     "start_time": "2025-10-13T21:28:26.583406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This is a call back function that draws the landmark results on the frame we pass it into the vision task so it knows what to do with the results\n",
    "def print_result(result: vision.HandLandmarkerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    global annotated_frame\n",
    "    annotated_frame = draw_landmarks_on_image(output_image.numpy_view(), result)"
   ],
   "id": "4dcc9ac7d799199c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T21:28:26.651651Z",
     "start_time": "2025-10-13T21:28:26.602048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"tasks/hand_landmarker.task\"\n",
    "base_options = python.BaseOptions(model_asset_path=model_path) #specify where the model/task we are using for this script is and add it to the configuration\n",
    "# Adding more configuration to for the task.\n",
    "# here we said using the hand landmarker task, look for up to 2 hands or less, using the LIVE_STREAM mode since it is a live video feed and then print the results to the frame\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options, num_hands=2, running_mode=vision.RunningMode.LIVE_STREAM,\n",
    "                                       result_callback=print_result)\n",
    "#then we create the detecor object using our configurations.\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "annotated_frame = None # lets us detect any changes in the frame after we run detection"
   ],
   "id": "44c08d04705a13df",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T21:28:26.673091Z",
     "start_time": "2025-10-13T21:28:26.668336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Utility function to draw images on the screen based on detection results.\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    import numpy as np\n",
    "    annotated_image = rgb_image.copy() # make a copy of the frame\n",
    "    # Loop through the number of hands and create a circle on the new frame for each one at its position\n",
    "    for hand_landmarks in detection_result.hand_landmarks:\n",
    "        for landmark in hand_landmarks:\n",
    "            x = int(landmark.x * annotated_image.shape[1])\n",
    "            y = int(landmark.y * annotated_image.shape[0])\n",
    "            cv2.circle(annotated_image, (x, y), 5, (0, 255, 0), -1)\n",
    "    return annotated_image # new image with the landmarks drawn."
   ],
   "id": "be50f0ecc4babef6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T21:30:41.837224Z",
     "start_time": "2025-10-13T21:28:26.699906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Reading From a video file\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "cv2.namedWindow('MediaPipe', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('MediaPipe', 960, 540)  # width, height\n",
    "\n",
    "timestamp = 0\n",
    "while cap.isOpened():\n",
    "    _, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    try:\n",
    "        #convert to RGB format\n",
    "        RGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        #convert the image to media pipe image\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=RGB)\n",
    "        # generate landmark data\n",
    "        detector.detect_async(mp_image, timestamp)\n",
    "        timestamp += 33\n",
    "\n",
    "        # checks which frame to draw. if there are no landmarks detected it draws the original frame\n",
    "        if annotated_frame is not None:\n",
    "            cv2.imshow('MediaPipe', cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)) # here we convert it back to rbg to avoid the blue tint\n",
    "            # cv2.imshow('MediaPipe', annotated_frame)\n",
    "        else:\n",
    "            cv2.imshow('MediaPipe', frame)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "ce67413a4b428af8",
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
